{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scsU06hozgeO"
      },
      "outputs": [],
      "source": [
        "import spacy # text analysis\n",
        "from spacy import displacy # tree plotting\n",
        "import pandas as pd # data-frame manipulation\n",
        "from tqdm.auto import tqdm # progress bar\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import seaborn as sns # plotting\n",
        "\n",
        "sns.set(context='paper', style='ticks', font_scale=1) # set the plot style"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the spaCy model for the English language:"
      ],
      "metadata": {
        "id": "tAysvFwfRVtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "zonCesw1Dr0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also load the book corpus:"
      ],
      "metadata": {
        "id": "k6uZlETxRZja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "harry_potter_corpus = pd.read_csv(\"https://raw.githubusercontent.com/\" +\n",
        "                                  \"alexis-raymond/NLP-HP-Books/refs/\" +\n",
        "                                  \"heads/main/data/processed/training_df.csv\")"
      ],
      "metadata": {
        "id": "rrWJTjW2MgBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that each row is storing one sentence and each sentence is annotated according to the book it is taken from."
      ],
      "metadata": {
        "id": "yo_bMFXqLXeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "harry_potter_corpus.head(10)"
      ],
      "metadata": {
        "id": "9veA3IWUM5xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Inspiration\n",
        "\n",
        "The contents of this practical are heavily inspired by this [paper](https://www.pnas.org/doi/10.1073/pnas.2319514121), so if you want to know more about this kind of work you are encouraged to read it."
      ],
      "metadata": {
        "id": "61dSFbvoP088"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Syntax in spaCy\n",
        "\n",
        "Besides from morphological analysis, spaCy can also be used to syntactic analyses. For instance, you can get the syntactic category of a word, or its part-of-speech (POS).\n",
        "\n",
        "Let's analyse the sentence \"John saw a man with a telescope\":\n"
      ],
      "metadata": {
        "id": "w5QbzywkDYkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"John saw a man with a telescope.\""
      ],
      "metadata": {
        "id": "kcKX9Blb4kJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the POS and lemmas of the word-forms in this sentence:"
      ],
      "metadata": {
        "id": "sgO1bmm0ExeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in nlp(text):\n",
        "    print(f'{token.text:{12}} {token.lemma_:{12}} {token.pos_}')"
      ],
      "metadata": {
        "id": "y4kKMFaDEu9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides from this, we can also look at the syntactic relationships between words in sentences using spaCy. However, it's not quite like binary trees that we saw in class. Instead, this package produces the so-called dependency trees:"
      ],
      "metadata": {
        "id": "2G0vyU1cFDd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nlp(text), style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "3MubWoHxFS3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This types of visualizations of syntax are called dependency trees, because the visualize the relationships between heads in dependents in a phrase, instead of grupping them together into nested structures. For us, the most important thing about this type of analysis is that it can detect the subject (nsubj) of sentence, as well as the direct object (dobj)."
      ],
      "metadata": {
        "id": "8ri0fLjyGHBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Which character has more agency?\n",
        "\n",
        "Let's pick three characters (Harry, Hermione and Ron) and, and find all of the sentences where they are the subject (nsubj) of a verb. First, we will need to find all the sentences that contain the words 'Harry', 'Hermione' and 'Ron'. For simplicity, we will strart by only looking at book 1."
      ],
      "metadata": {
        "id": "dqe45TFzMFse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book_1 = harry_potter_corpus[harry_potter_corpus['book'] == 1]\n",
        "book_1.shape"
      ],
      "metadata": {
        "id": "xdkbA8DqMAv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find all the sentences with at least one of the three characters. We will be using the pandas `str` functionality to do this:"
      ],
      "metadata": {
        "id": "Hkp7netVNKUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = ['Harry',\n",
        "              'Hermione',\n",
        "              'Ron']\n",
        "\n",
        "sents = book_1[book_1['sentence'].str.contains('|'.join(characters))]\n",
        "sents.shape"
      ],
      "metadata": {
        "id": "N-Cn6XRrNIFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, 1/3 of the first book sentences contain the names of the characters that we are interested in. First, let's start by counting the percentage of sentences in which every character appears individually:"
      ],
      "metadata": {
        "id": "3dnNAAjmNalX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts_frec = dict()\n",
        "\n",
        "for sent in sents['sentence'].values:\n",
        "    for char in characters:\n",
        "        if char in sent:\n",
        "            if char in counts_frec:\n",
        "                counts_frec[char] += 1\n",
        "            else:\n",
        "                counts_frec[char] = 1"
      ],
      "metadata": {
        "id": "2ADz2YMWL-8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, Harry is one of the most frequent characters."
      ],
      "metadata": {
        "id": "2BmBU3IXN2xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts_frec"
      ],
      "metadata": {
        "id": "SIvZ1rt_L_mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's process those sentences with spacy, and find the sentences where one of the characters has the `nsubj` relationship with the verb. To do this, we first need to get the dependency of the subject, then look at it's ancestors, and retrieve the verb that governs it."
      ],
      "metadata": {
        "id": "VR-seRjLOAXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the sentences in which the character is nsubj, but also record the verb\n",
        "subjs = []\n",
        "\n",
        "for sent in tqdm(sents['sentence'].values):\n",
        "    doc = nlp(sent)\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'nsubj' and token.text in characters:\n",
        "          for ancestor in token.ancestors:\n",
        "            if ancestor.pos_ == 'VERB':\n",
        "              subjs.append((token.text, ancestor.lemma_))"
      ],
      "metadata": {
        "id": "zM5ylTUeOJSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the results:"
      ],
      "metadata": {
        "id": "cmTjMg5rO4py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjs[:10]"
      ],
      "metadata": {
        "id": "OTVXXH0qL_s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's convert this to a dataframe:"
      ],
      "metadata": {
        "id": "49yiEVPQO9Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjects_df = pd.DataFrame(subjs, columns=['subject', 'verb'])\n",
        "subjects_df.head(10)"
      ],
      "metadata": {
        "id": "Xq91cm2OO_Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's count the number of times each of the characters is a subject in a sentence. First, we extract the counts:co"
      ],
      "metadata": {
        "id": "stHI6B3tPRp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = subjects_df['subject'].value_counts()\n",
        "counts"
      ],
      "metadata": {
        "id": "Co_H7LJLP2GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we plot them:"
      ],
      "metadata": {
        "id": "uDQPJHmEP649"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=counts.index, y=counts.values)\n",
        "plt.title('Number of times a character is a subject in a sentence')\n",
        "plt.xlabel('Character')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8RM5ITbHP8g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this plot is providing some information, it is a bit meaningless, since harry appears the most in all of the sentnces anyways, let's divide this number by the number of total appearances to get the percentage of `subjecthood`:"
      ],
      "metadata": {
        "id": "AsOpB4OZQDeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts_df = pd.DataFrame(counts).reset_index()\n",
        "counts_df.columns = ['subject', 'count']\n",
        "\n",
        "counts_df['count_frec'] = counts_df['subject'].apply(lambda x: counts_frec[x])\n",
        "counts_df['percentage_nsubj'] = counts_df['count'] / counts_df['count_frec']\n",
        "\n",
        "counts_df.head(10)"
      ],
      "metadata": {
        "id": "Xd4ItVDJQPau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the percentages, what this plot can tell you about the first book?"
      ],
      "metadata": {
        "id": "m7k34eFHQruH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8wnButrbP5hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the `counts_df` database, we also stored the verbs corresponding to each of the three characters -- which verbs are the most frequently used with which character?"
      ],
      "metadata": {
        "id": "FVkPA62uialC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "h9CMYUcuilHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Agency surplus\n",
        "\n",
        "The interesting thing about spaCy is that we can not only detect subjects, but arso objects, i.e. participants on which the action is directed.\n",
        "\n",
        "For instance, in the sentence \"John saw a man with a telescope\", \"a man\" or \"a man with a telescope\" are objects of the verb \"see\", depending on the interpretation.\n",
        "\n",
        "We can see this on the dependency tree, where the objects of a verb are usually labeled as labelled `dobj`:"
      ],
      "metadata": {
        "id": "UfQ2HbOBRCU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nlp(text), style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "qAHVsRt7jJdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify our code from above to also record cases in which one of the characters is an object. For that, we will add a new list `objs`, in which we will store the verbs in which the corresponding character appears as `dobj`:"
      ],
      "metadata": {
        "id": "OU9SaYGekIz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the sentences in which the character is nsubj, but also record the verb\n",
        "subjs = []\n",
        "objs = []\n",
        "\n",
        "for sent in tqdm(sents['sentence'].values):\n",
        "    doc = nlp(sent)\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'nsubj' and token.text in characters:\n",
        "          for ancestor in token.ancestors:\n",
        "            if ancestor.pos_ == 'VERB':\n",
        "              subjs.append((token.text, ancestor.lemma_))\n",
        "        elif token.dep_ == 'dobj' and token.text in characters:\n",
        "          for ancestor in token.ancestors:\n",
        "            if ancestor.pos_ == 'VERB':\n",
        "              objs.append((token.text, ancestor.lemma_))"
      ],
      "metadata": {
        "id": "c3ib3O8SkOMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's combine all of this into one dataframe:"
      ],
      "metadata": {
        "id": "nLaEnfDilG8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's convert both lists to dataframes\n",
        "subjs_df = pd.DataFrame(subjs, columns=['character', 'verb'])\n",
        "objs_df = pd.DataFrame(objs, columns=['character', 'verb'])\n",
        "\n",
        "## add columns with roles\n",
        "subjs_df['role'] = 'subject'\n",
        "objs_df['role'] = 'object'\n",
        "\n",
        "# concatenate the dataframes\n",
        "agency_df = pd.concat([subjs_df, objs_df])\n",
        "\n",
        "agency_df.head(5)"
      ],
      "metadata": {
        "id": "Vd_zC6t6kH_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compute the number of times that the characters were either subjects or objects, and we can convert these values to percentages according to the total number of sentences in which every character is either a subject or an object:"
      ],
      "metadata": {
        "id": "nuPw0YmClXXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "character_summary = agency_df.groupby('character')['role'].value_counts().reset_index()\n",
        "character_summary"
      ],
      "metadata": {
        "id": "32Y581_llpwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's convert the counts into percentages by summing them for each character and then dividing each entry by this sum:"
      ],
      "metadata": {
        "id": "1Gix5FJfl8iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "character_summary['percentage'] = character_summary.groupby('character')['count'].transform(lambda x: x / x.sum())\n",
        "character_summary"
      ],
      "metadata": {
        "id": "I9bYlbUPmD1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute the agency surplus by applying the following equation to each character:\n",
        "\n",
        "$$\\text{Agency surplus} = count(object) - count(object)$$\n",
        "\n",
        "If this value is positive, the character is more frequently and object, but if it's negative, the character is more likely to be a subject, i.e. an active participant."
      ],
      "metadata": {
        "id": "MJAIbX0fmbeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ####"
      ],
      "metadata": {
        "id": "Z0tXig2ul63c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finaly, can you extend this analysis to all of the 6 books and compare the agency surprlus of each character? To do this, you would need to write a function that takes the set of sentences in a book, and returns the character info with their corresponding agency surplus."
      ],
      "metadata": {
        "id": "e5hTQQriOoKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ####"
      ],
      "metadata": {
        "id": "h27rQou9PQ7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}